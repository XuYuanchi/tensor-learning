{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Intuitive Understanding of Randomized Singular Value Decomposition</h1>\n",
    "\n",
    "<h4 align = \"center\">A Python Implementation of SVD with Randomized Linear Algebra</h4>\n",
    "\n",
    "Matrix decomposition is a foundational tool for some critical applications like data compression, dimensionality reduction, and sparsity learning. In many cases, for purposes of approximating a data matrix by a low-rank structure, the Singular Value Decomposition (SVD) is the best choice. However, the accurate and efficient SVD of large-scale datasets is computationally challenging. To resolve the SVD in this situation, there are many algorithms have been developed by applying randomized linear algebra. One of the most important algorithms is randomized SVD, which is competitively efficient for factorizing any matrix with a relatively low rank.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img align=\"middle\" src=\"../images/svd_history.png\" width=\"750\" />\n",
    "</p>\n",
    "\n",
    "<center><b>Figure 1</b>: A timeline of major SVD developments. (The picture is from [2]).</center>\n",
    "\n",
    "\n",
    "This post will introduce the preliminary and essential idea of the randomized SVD. To help readers gain a better understanding of randomized SVD, we also provide the corresponding Python implementation in this post.\n",
    "\n",
    "> For reproducing this notebook, please clone or download the **tensor-learning** repository ([https://github.com/xinychen/tensor-learning](https://github.com/xinychen/tensor-learning)) on your computer first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### SVD Formula\n",
    "\n",
    "As you may already know, SVD is one of the most important decomposition formula in linear algebra. For any given matrix $\\boldsymbol{A}$, SVD has the form of\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top\n",
    "\\end{equation}\n",
    "where the matrices $\\boldsymbol{U}$ and $\\boldsymbol{V}$ consist of left and right singular vectors, respectively. The diagonal entries of $\\boldsymbol{\\Sigma}$ are singular values.\n",
    "\n",
    "### A Small Matrix Example\n",
    "\n",
    "We will give a sufficiently detailed understanding with a small worked example. The problem is a simple SVD of 3-by-3 matrix, i.e.,\n",
    "$$\\boldsymbol{A}=\\left(\\begin{array}{cccc}\n",
    "1 & 3 & 2 \\\\\n",
    "5 & 3 & 1 \\\\\n",
    "3 & 4 & 5 \\\\\n",
    "\\end{array}\\right)\\in\\mathbb{R}^{3\\times 3}.$$\n",
    "\n",
    "Take this 3-by-3 matrix for example, we can compute the SVD by using `numpy.linalg.svd()` in Python. Let us take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left singular vectors:\n",
      "[[-0.37421754  0.28475648 -0.88253894]\n",
      " [-0.56470638 -0.82485997 -0.02669705]\n",
      " [-0.7355732   0.48838486  0.46948087]]\n",
      "\n",
      "Singular values:\n",
      "[9.34265841 3.24497827 1.08850813]\n",
      "\n",
      "Right singular vectors:\n",
      "[[-0.57847229 -0.61642675 -0.53421706]\n",
      " [-0.73171177  0.10269066  0.67383419]\n",
      " [ 0.36051032 -0.78068732  0.51045041]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 3, 2],\n",
    "              [5, 3, 1],\n",
    "              [3, 4, 5]])\n",
    "u, s, v = np.linalg.svd(A, full_matrices = 0)\n",
    "print('Left singular vectors:')\n",
    "print(u)\n",
    "print()\n",
    "print('Singular values:')\n",
    "print(s)\n",
    "print()\n",
    "print('Right singular vectors:')\n",
    "print(v)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the singular values are **9.3427**, **3.2450**, and **1.0885**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized SVD\n",
    "\n",
    "### Essential Idea\n",
    "\n",
    "Randomized SVD can be broken into three steps. For any given $m$-by-$n$ matrix $\\boldsymbol{A}$, if we impose a target rank $k$ with $k < \\min\\{m, n\\}$, then the first step is to\n",
    "\n",
    "- 1) generate a Gaussian random matrix $\\boldsymbol{Î©}$ with size of $n$-by-$k$,\n",
    "- 2) compute a new $m$-by-$k$ matrix $\\boldsymbol{Y}$,\n",
    "- and 3) apply QR decomposition to the matrix $\\boldsymbol{Y}$.\n",
    "\n",
    "Note that the first step needs to return the $m$-by-$k$ matrix $\\boldsymbol{Q}$.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img align=\"middle\" src=\"../images/rsvd_step1.png\" width=\"600\" />\n",
    "</p>\n",
    "\n",
    "<center><b>Figure 2</b>: The first step of randomized SVD. (The picture is from [2]).</center>\n",
    "\n",
    "Then, the second step as shown in Figure 3 is to\n",
    "\n",
    "- 4) derive a $k$-by-$n$ matrix $\\boldsymbol{B}$ by multiplying $\\boldsymbol{Q}^\\top$ and the matrix $\\boldsymbol{A}$ together, i.e., $\\boldsymbol{B}=\\boldsymbol{Q}^\\top\\boldsymbol{A}$,\n",
    "- and 5) compute the SVD of the matrix $\\boldsymbol{B}$. Here, instead of computing the SVD of the original matrix $\\boldsymbol{A}$, $\\boldsymbol{B}$ is a smaller matrix to work with.\n",
    "\n",
    "Note that the singular values (i.e., $\\boldsymbol{\\Sigma}$)and right singular vectors (i.e., $\\boldsymbol{V}$) of the matrix $\\boldsymbol{B}$ are also the singular values and right singular vectors of the matrix $\\boldsymbol{A}$.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img align=\"middle\" src=\"../images/rsvd_step2.png\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "<center><b>Figure 3</b>: The second and third steps of randomized SVD. (The picture is from [2]).</center>\n",
    "\n",
    "As shown in Figure 3, if we combine the matrix $\\boldsymbol{Q}$ derived in the first step with the left singular vectors of $\\boldsymbol{B}$, we can get the left singular vectors (i.e., $\\boldsymbol{U}$) of the matrix $\\boldsymbol{A}$ in the third step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Small Matrix Example\n",
    "\n",
    "Even though we have learned the essential idea of randomized SVD in above, it would not be really clear if there is no intuitive example. To this end, we follow the aforementioned small matrix SVD.\n",
    "\n",
    "First, let us try to write a Python function for randomized SVD. Here, we will use two Numpy functions, i.e., `np.linalg.qr()` and `np.linalg.svd()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rsvd(A, Omega):\n",
    "    Y = A @ Omega\n",
    "    Q, _ = np.linalg.qr(Y)\n",
    "    B = Q.T @ A\n",
    "    u_tilde, s, v = np.linalg.svd(B, full_matrices = 0)\n",
    "    u = Q @ u_tilde\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test it with 3-by-3 matrix (`rank = 2` for indicating $k$ with $k < \\min(m, n)$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left singular vectors:\n",
      "[[ 0.38070859  0.60505354]\n",
      " [ 0.56830191 -0.74963644]\n",
      " [ 0.72944767  0.26824507]]\n",
      "\n",
      "Singular values:\n",
      "[9.34224023 3.02039888]\n",
      "\n",
      "Right singular vectors:\n",
      "[[ 0.57915029  0.61707064  0.53273704]\n",
      " [-0.77420021  0.21163814  0.59650929]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "A = np.array([[1, 3, 2],\n",
    "              [5, 3, 1],\n",
    "              [3, 4, 5]])\n",
    "rank = 2\n",
    "Omega = np.random.randn(A.shape[1], rank)\n",
    "u, s, v = rsvd(A, Omega)\n",
    "print('Left singular vectors:')\n",
    "print(u)\n",
    "print()\n",
    "print('Singular values:')\n",
    "print(s)\n",
    "print()\n",
    "print('Right singular vectors:')\n",
    "print(v)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the singular values of this matrix are **9.3427**, **3.2450**, and 1.0885. In this case, randomized SVD has the first two singular values as **9.3422** and **3.0204**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first singular values computed by these two SVD algorithms are extremely close. However, the second singular value of randomized SVD has a slight bias. Is there any other method to improve this result? And how?\n",
    "\n",
    "The answer is yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized SVD with Power Iteration\n",
    "\n",
    "To improve the quality of randomized SVD, power iteration method can be used directly. For more detail about power iteration, please see the page 39 in [1] and there is also a Matlab implementation in the page 40.\n",
    "\n",
    "In the following Python codes, `power_iteration()` is the function for computing the $m$-by-$k$ matrix $\\boldsymbol{Y}$ iteratively (the default `power_iter` is 3) and then derive the $m$-by-$k$ matrix $\\boldsymbol{Q}$ by QR decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def power_iteration(A, Omega, power_iter = 3):\n",
    "    Y = A @ Omega\n",
    "    for q in range(power_iter):\n",
    "        Y = A @ (A.T @ Y)\n",
    "    Q, _ = np.linalg.qr(Y)\n",
    "    return Q\n",
    "\n",
    "def rsvd(A, Omega):\n",
    "    Q = power_iteration(A, Omega)\n",
    "    B = Q.T @ A\n",
    "    u_tilde, s, v = np.linalg.svd(B, full_matrices = 0)\n",
    "    u = Q @ u_tilde\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our new `rsvd()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left singular vectors:\n",
      "[[ 0.37421757  0.28528579]\n",
      " [ 0.56470638 -0.82484381]\n",
      " [ 0.73557319  0.48810317]]\n",
      "\n",
      "Singular values:\n",
      "[9.34265841 3.24497775]\n",
      "\n",
      "Right singular vectors:\n",
      "[[ 0.57847229  0.61642675  0.53421706]\n",
      " [-0.73178429  0.10284774  0.67373147]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "A = np.array([[1, 3, 2],\n",
    "              [5, 3, 1],\n",
    "              [3, 4, 5]])\n",
    "rank = 2\n",
    "Omega = np.random.randn(A.shape[1], rank)\n",
    "u, s, v = rsvd(A, Omega)\n",
    "print('Left singular vectors:')\n",
    "print(u)\n",
    "print()\n",
    "print('Singular values:')\n",
    "print(s)\n",
    "print()\n",
    "print('Right singular vectors:')\n",
    "print(v)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that:\n",
    "\n",
    "- Singular values of SVD are: **9.3427**, **3.2450**, and 1.0885.\n",
    "- Singular values of randomized SVD without power iteration are: **9.3422** and **3.0204**.\n",
    "- Singular values of randomized SVD with power iteration are: **9.3427** and **3.2450**.\n",
    "\n",
    "As you can see, the randomized SVD with power iteration provides extremely accurate singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this post, you discovered the randomized linear algebra method for SVD. Specifically, you learned:\n",
    "\n",
    "- The essential idea of randomized SVD.\n",
    "- How to implement randomized SVD in Python.\n",
    "\n",
    "Do you have any question? Ask your question by creating an issue at the **tensor-learning** repository ([https://github.com/xinychen/tensor-learning](https://github.com/xinychen/tensor-learning)) and I will do my best to answer. If you find these codes useful, please star (â) this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Steven L. Brunton, J. Nathan Kutz (2019). Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control. Page 37â41.\n",
    "\n",
    "[2] N. Benjamin Erichson, Sergey Voronin, Steven L. Brunton, J. Nathan Kutz (2016). Randomized Matrix Decompositions Using R. arXiv:1608.02148. [[PDF](https://arxiv.org/pdf/1608.02148.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
