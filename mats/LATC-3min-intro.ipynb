{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Understanding of Low-Rank Autoregressive Tensor Completion</h1>\n",
    "\n",
    "<h4 align = \"center\">3-Minute Introduction to Motivation, Essential Idea, and Modeling Framework</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highlight 1: Why should we propose such autoregressive process and low-rank structure combined forecasting model?\n",
    "\n",
    "Time series prediction has been a long-standing research topic and an essential application in many domains. Modern time series collected from sensor networks (e.g., energy consumption and traffic flow) are often large-scale and incomplete with considerable corruption and missing values, making it difficult to perform accurate predictions. In this project, we propose a **Low-Rank Autoregressive Tensor Completion (LATC)** model in the following article:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2020). <b>Low-Rank Autorgressive Tensor Completion for Multivariate Time Series Forecasting</b>. arXiv.2006.10436. <a href=\"https://arxiv.org/pdf/2006.10436.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>\n",
    "\n",
    "In this article, we develop a **LATC imputer/predictor** for multivariate time series imputation/forecasting. The key of LATC is to transform the original multivariate time series matrix (e.g., sensor$\\times$time point) to a third-order tensor structure (e.g., sensor$\\times$time of day$\\times$day) by introducing an additional temporal dimension, which allows us to model the inherent rhythms and seasonality of time series as global patterns. With the tensor structure, we can transform the time series prediction and missing data imputation problems into a universal low-rank tensor completion problem. Besides minimizing tensor rank, we also integrate a novel autoregressive norm on the original matrix representation into the objective function. The two components serve different roles. The low-rank structure allows us to effectively capture the global consistency and trends across all the three dimensions (i.e., similarity among sensors, similarity of different days, and current time *v.s.* the same time of historical days). The autoregressive norm can better model the local temporal trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highlight 2: How does this model capture the data behaviour of multivariate time series?\n",
    "\n",
    "The formulation takes into account both autoregression error minimization and tensor nuclear norm minimization:\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    \\min _{\\boldsymbol{\\mathcal{X}},\\boldsymbol{Z},\\boldsymbol{A}}~&\\|\\boldsymbol{\\mathcal{X}}\\|_{*}+\\lambda\\|\\boldsymbol{Z}\\|_{\\boldsymbol{A},\\mathcal{H}} \\\\ \\text { s.t.}~&\\left\\{\\begin{array}{l}\\boldsymbol{\\mathcal{X}}=\\mathcal{Q}\\left(\\boldsymbol{Z}\\right), \\\\ \\mathcal{P}_{\\Omega}(\\boldsymbol{Z})=\\mathcal{P}_{\\Omega}(\\boldsymbol{Y}), \\\\ \\end{array}\\right. \\\\\n",
    "    \\end{aligned}\n",
    "    \\label{lrtc_ar}\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{Y}\\in\\mathbb{R}^{M\\times (IJ)}$ is the partially observed time series matrix. In the objective function, $\\|\\cdot\\|_{*}$ denotes the tensor nuclear norm; $\\|\\cdot\\|_{\\boldsymbol{A},\\mathcal{H}}$ denotes the autoregressive norm of multivariate time series matrix which holds the following definition:\n",
    "\\begin{equation} \n",
    "    \\|\\boldsymbol{Z}\\|_{\\boldsymbol{A},\\mathcal{H}}=\\sum_{m,t}(z_{m,t}-\\sum_{i}a_{m,i}z_{m,t-h_i})^{2},\n",
    "    \\label{auto_norm}\n",
    "\\end{equation}\n",
    "for matrix $\\boldsymbol{Z}$ with a lag set $\\mathcal{H}$ and coefficient matrix $\\boldsymbol{A}$. As shown in Figure 1, we provide an intuitive understanding of the proposed LATC model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img align=\"middle\" src=\"../images/predictor-explained.png\" width=\"700\" />\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<b>Figure 1</b>: Illustration of our proposed Low-Rank Tensor Completion (LATC) imputer/predictor with a prediction window $\\tau$ (green nodes: observed values; white nodes: missing values; red nodes/panel: prediction; blue panel: training data to construct the tensor).\n",
    "</center>\n",
    "\n",
    "Python codes for reproducing experiments are provided in the [**../mats**](https://github.com/xinychen/tensor-learning/tree/master/mats) folder. Since these Python codes were written on the Jupyter Notebook, you could also view them on the nbviewer. Please open\n",
    "\n",
    "- [LATC-imputer.ipynb](https://nbviewer.jupyter.org/github/xinychen/tensor-learning/blob/master/mats/LATC-imputer.ipynb)\n",
    "- [LATC-predictor.ipynb](https://nbviewer.jupyter.org/github/xinychen/tensor-learning/blob/master/mats/LATC-predictor.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hightlight 3: How to generalize this model to large-scale and high-dimensional data?\n",
    "\n",
    "We are trying to solve this problem now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
